{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj-HaJoUMk6U"
      },
      "source": [
        "**RentTheRunWay**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRvJ6iIGMu2q"
      },
      "source": [
        "**● Import the required libraries and load the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62H4cuYjMzQ5"
      },
      "source": [
        "1. Load the required libraries and read the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGDoCVJlNAdy"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk6JtQ-kPWMP"
      },
      "outputs": [],
      "source": [
        "# If using Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkKc2MvQNKIJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/renttherunway.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms9rf4DPP0ZG"
      },
      "source": [
        "**2. Check the first few samples, shape, info of the data and try to familiarize yourself with different features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlXvL_htP4Yd"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0uZmESpQEWH"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF5USAHSQEmS"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZZwMs6Uwc9W"
      },
      "source": [
        "- There are 190K instances and 15 columns.\n",
        "- We can observe the missing values in the dataset.\n",
        "- There are around 10 object type variables and 5 numerical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmg80FPgQZMb"
      },
      "source": [
        "**● Data cleansing and Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IawDGOhQcn0"
      },
      "source": [
        "3. Check if there are any duplicate records in the dataset? If any, drop them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSDbH0QoQnHD"
      },
      "outputs": [],
      "source": [
        "## checking the presence of duplicate records\n",
        "len(df[df.duplicated()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBojuvQAQvAB"
      },
      "source": [
        "4. Drop the columns which you think redundant for the analysis.(Hint: drop columns like ‘id’, ‘review’)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJsuy0izQw6H"
      },
      "outputs": [],
      "source": [
        "## dropping the redundant columns from the dataset.\n",
        "df.drop(['user_id', 'item_id', 'review_text', 'review_summary', 'review_date'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K2ynzgpQ4Ad"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIMFyxRwQ94n"
      },
      "source": [
        "5. Check the column 'weight', Is there any presence of string data? If yes, remove the string data and convert to float. (Hint: 'weight' has the suffix as lbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpEXSYUXRAnU"
      },
      "outputs": [],
      "source": [
        "df['weight'] = df['weight'].str.replace('lbs','').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2o5GOuuRFj7"
      },
      "outputs": [],
      "source": [
        "df['weight'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aaO5zAjRbIR"
      },
      "source": [
        "6. Check the unique categories for the column 'rented for' and group 'party: cocktail' category with 'party'. (\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW_oub6MQzOR"
      },
      "outputs": [],
      "source": [
        "df['rented for'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q109q2ymRc97"
      },
      "outputs": [],
      "source": [
        "## grouping 'party: cocktail' category with the 'party'.\n",
        "df['rented for'] = df['rented for'].str.replace('party: cocktail','party')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVgqgogfRdGi"
      },
      "outputs": [],
      "source": [
        "## recheck unique values after grouping\n",
        "df['rented for'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAyOj0hxRda1"
      },
      "source": [
        "7. The column 'height' is in feet with a quotation mark, Convert to inches with float datatype\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYzmixTdRqBc"
      },
      "outputs": [],
      "source": [
        "## Removing quotation marks\n",
        "df['height'] = df['height'].str.replace(\"'\",'')\n",
        "df['height'] = df['height'].str.replace('\"','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOHWOVZHSAfc"
      },
      "outputs": [],
      "source": [
        "## Convert the feet to inches and convert the datatype to float\n",
        "df['height'] = (df['height'].str[:1].astype(float)*12 + df['height'].str[1:].astype(float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9wvG339SEAq"
      },
      "outputs": [],
      "source": [
        "df['height'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-nFh85CSMLe"
      },
      "source": [
        "8. Check for missing values in each column of the dataset? If it exists, impute them with appropriate methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jScz9uxXTKKY"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()/len(df)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEHDDB8HTV4w"
      },
      "outputs": [],
      "source": [
        "## Lets treat categoricak columns with mode imputation technique.\n",
        "for col in ['bust size','rented for','body type','category']:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLoAOEBATWDI"
      },
      "outputs": [],
      "source": [
        "## Lets treat categoricak columns with mode imputation technique.\n",
        "for col in ['bust size','rented for','body type','category']:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2GOwry0yxjL"
      },
      "outputs": [],
      "source": [
        "## lets recheck the missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDS6tO8rTi2J"
      },
      "source": [
        "9. Check the statistical summary for the numerical and categorical columns and write your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9c1FmUwTsAu"
      },
      "outputs": [],
      "source": [
        "## let us check the statistical summary for the numerical columns\n",
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MofnBi_1y8t5"
      },
      "outputs": [],
      "source": [
        "## let us check the statistical summary for the categorical columns.\n",
        "df.describe(include='O').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxV2lKRIy-Es"
      },
      "source": [
        "- The average weight of the customer is around 137lbs.\n",
        "- The average rating is around 9.\n",
        "- The maximum height of the customer is 78 inches.\n",
        "- The maximum standarized size of the product is 58.\n",
        "- The age range is 0 to 117.\n",
        "- Note we can see the min age is 0 we need to impute it with appropriate value and the maximun age we need to cap it to Upperlimit.\n",
        "- Most of the customers rented the product for wedding and the most appeared product category is as dress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUHA96uATswp"
      },
      "source": [
        "10. Are there outliers present in the column age? If yes, treat them with the appropriate method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DmZpXToT2D9"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(df['age'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZH5RKTiT6GB"
      },
      "outputs": [],
      "source": [
        "## lets treat the outliers in the column age using capping techinque\n",
        "\n",
        "df['age'] = pd.DataFrame(np.where(df['age']>=100,100,df['age']))\n",
        "df['age'] = pd.DataFrame(np.where(df['age']<=20,20,df['age']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbNpYKJXT6Mr"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(df['age'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyslvOgtT6UX"
      },
      "outputs": [],
      "source": [
        "## after applying capping technique for the column age, there might be some presence of missing values in columns age, So drop them\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYVa-Td5UQKz"
      },
      "source": [
        "11. Check the distribution of the different categories in the column 'rented for' using appropriate plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7Cz7uR-T6jt"
      },
      "outputs": [],
      "source": [
        "## let us check the distribution of the column rented for\n",
        "sns.countplot(df['rented for'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWAu-X-Rza0f"
      },
      "source": [
        "- We can see that the most of the customers rented the product for the wedding followed by party and formal affair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_r4Cn4TzbJ9"
      },
      "outputs": [],
      "source": [
        "## Let us make a copy of the cleaned dataset before encoding and standardizing the columns\n",
        "dfc1 = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpDJgVNCuAl4"
      },
      "source": [
        "**● Data Preparation for model building:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T04rQEjOuEcN"
      },
      "source": [
        "12. Encode the categorical variables in the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAWlj509uIyD"
      },
      "outputs": [],
      "source": [
        "## Encoding categorical variables using label encoder\n",
        "\n",
        "## select object datatype variables\n",
        "object_type_variables = [i for i in df.columns if df.dtypes[i] == object]\n",
        "object_type_variables\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "def encoder(df):\n",
        "    for i in object_type_variables:\n",
        "        q = le.fit_transform(df[i].astype(str))\n",
        "        df[i] = q\n",
        "        df[i] = df[i].astype(int)\n",
        "encoder(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XnXKhDquI8d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n91K90NlztPT"
      },
      "source": [
        "### 13. Standardize the data, so that the values are within a particular range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB70-W0OuJF0"
      },
      "outputs": [],
      "source": [
        "## Tranforming the data using minmax scaling approach so that the values range will be 1.\n",
        "\n",
        "mm = MinMaxScaler()\n",
        "\n",
        "df.iloc[:,:] = mm.fit_transform(df.iloc[:,:])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oDkEB00uJO5"
      },
      "outputs": [],
      "source": [
        "## Let us make a copy of the cleaned dataset after encoding and standardizing the columns.\n",
        "dfc2 = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vGqGLcVz4R4"
      },
      "source": [
        "### 14. Apply PCA on the above dataset and determine the number of PCA components to be used so that 90-95% of the variance in data is explained by the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IbQFoNjz8z5"
      },
      "outputs": [],
      "source": [
        "## step1: Calculate the covariance matrix.\n",
        "cov_matrix = np.cov(df.T)\n",
        "cov_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lze4BbUz9OP"
      },
      "outputs": [],
      "source": [
        "## step2: Calculate the eigen values and eigen vectors.\n",
        "eig_vals, eig_vectors = np.linalg.eig(cov_matrix)\n",
        "print('eigein vals:','\\n',eig_vals)\n",
        "print('\\n')\n",
        "print('eigein vectors','\\n',eig_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63ptCjM90FJc"
      },
      "outputs": [],
      "source": [
        "## step3: Scree plot.\n",
        "total = sum(eig_vals)\n",
        "var_exp = [(i/total)*100 for i in sorted(eig_vals,reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "print('Explained Variance: ',var_exp)\n",
        "print('Cummulative Variance Explained: ',cum_var_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXcw5niW0IKE"
      },
      "outputs": [],
      "source": [
        "## Scree plot.\n",
        "plt.bar(range(10),var_exp,align='center',color='lightgreen',edgecolor='black',label='Explained Variance')\n",
        "plt.step(range(len(var_exp)),cum_var_exp,where='mid',color='red',label='Cummulative Explained Variance') # Use len(var_exp) here as well for consistency\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explianed Variance ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x824HD3R0Mnt"
      },
      "source": [
        "- We can observe from the above scree plot the first 6 principal components are explaining the about 90-95% of the variation, So we can choose optimal number of principal components as 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAiVraDN0x93"
      },
      "source": [
        "### 15. Apply K-means clustering and segment the data (You may use original data or PCA transformed data)\n",
        "- a. Find the optimal K Value using elbow plot for KMeans clustering.\n",
        "- b. Build a Kmeans clustering model using the obtained optimal K value from the elbow plot.\n",
        "- c. Compute silhoutte score for evaluating the quality of the Kmeans clustering technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuU1OJQu0zmS"
      },
      "outputs": [],
      "source": [
        "## Using the dimensions obtainted from the PCA to apply clustering.(i.e, 6)\n",
        "pca = PCA(n_components=6)\n",
        "\n",
        "pca_df = pd.DataFrame(pca.fit_transform(df),columns=['PC1','PC2','PC3','PC4','PC5','PC6'])\n",
        "pca_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvKo-g3Y03pH"
      },
      "source": [
        "- These are the new dimensions obtained from the application of PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wISGdKD09Nz"
      },
      "source": [
        "**#### Kmeans clustering (using the PCA tranformed data)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cVaP3jD1Cgw"
      },
      "outputs": [],
      "source": [
        "## finding optimal K value by KMeans clustering using Elbow plot.\n",
        "cluster_errors = []\n",
        "cluster_range = range(2,15)\n",
        "for num_clusters in cluster_range:\n",
        "    clusters = KMeans(num_clusters,random_state=100)\n",
        "    clusters.fit(pca_df)\n",
        "    cluster_errors.append(clusters.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck1uderP1Col"
      },
      "outputs": [],
      "source": [
        "## creataing a dataframe of number of clusters and cluster errors.\n",
        "cluster_df = pd.DataFrame({'num_clusters':cluster_range,'cluster_errors':cluster_errors})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfo3KIkC1CwJ"
      },
      "outputs": [],
      "source": [
        "## Elbow plot.\n",
        "plt.figure(figsize=[15,5])\n",
        "plt.plot(cluster_df['num_clusters'],cluster_df['cluster_errors'],marker='o',color='b')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpNwHQU71J0a"
      },
      "source": [
        "- From the above elbow plot we can see at the cluster K=3, the inertia significantly decreases . Hence we can select our optimal clusters as K=3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCpUhd__1MxL"
      },
      "outputs": [],
      "source": [
        "## Applying KMeans clustering for the optimal number of clusters obtained above.\n",
        "kmeans = KMeans(n_clusters=3, random_state=100)\n",
        "kmeans.fit(pca_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99oiYMPW1UXf"
      },
      "outputs": [],
      "source": [
        "## creating a dataframe of the labels.\n",
        "label = pd.DataFrame(kmeans.labels_,columns=['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LkuPwpv19FB"
      },
      "outputs": [],
      "source": [
        "## joining the label dataframe to the pca_df dataframe.\n",
        "kmeans_df = pca_df.join(label)\n",
        "kmeans_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LOrf7H72COY"
      },
      "outputs": [],
      "source": [
        "kmeans_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dVR7IwQV2CWd"
      },
      "outputs": [],
      "source": [
        "## finding optimal clusters through silhoutte score\n",
        "from sklearn.metrics import silhouette_score\n",
        "for i in range(2,15):\n",
        "    kmeans = KMeans(i,random_state=100)\n",
        "    kmeans.fit(pca_df)\n",
        "    labels = kmeans.predict(pca_df)\n",
        "    print(i,silhouette_score(pca_df,labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra_c1tDL2GbK"
      },
      "source": [
        "- Above from elbow plot we chose optimal K value as 3 and we built a Kmeans clustering model.\n",
        "- From the silhoutte score we can observe the for clusters 2 and 3 the score is higher. We can build Kmeans clustering model using the optimal K value as either 2 or 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoPfJf8P2J85"
      },
      "source": [
        "### 16. Apply Agglomerative clustering and segement the data.  (You may use original data or PCA transformed data)\n",
        "- a. Find the optimal K Value using dendrogram for Agglomerative clustering.\n",
        "- b. Build a Agglomerative clustering model using the obtained optimal K value from observed from dendrogram.\n",
        "- c. Compute silhoutte score for evaluating the quality of the Agglomerative clustering technique.\n",
        "\n",
        "(Hint: Take a sample of the dataset for agglomerative clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl-qnc5k2KF8"
      },
      "source": [
        "#### Agglomerative clustering (using original data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4QYqovY2WUn"
      },
      "outputs": [],
      "source": [
        "## Let us use the dfc2 for this (a copy of the cleaned dataset after encoding and data standardization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcb6LhJ02W91"
      },
      "outputs": [],
      "source": [
        "## Since dataset is huge plotting dendrogram might be time consuming.\n",
        "## Let us take a sample of the dataset. (since the dataset is huge around 2 lakh rows, let take a sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWzVQS3k2deD"
      },
      "outputs": [],
      "source": [
        "## Taking a sample of 50K rows from the dfc2 dataframe using random sampling technique provided by pandas\n",
        "\n",
        "## Storing it in the new dataframe called 'dfc3'\n",
        "dfc3 = dfc2.sample(n=50000)\n",
        "\n",
        "## reseting the index\n",
        "dfc3.reset_index(inplace=True,drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPvNnSGS2doZ"
      },
      "outputs": [],
      "source": [
        "dfc3.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Hy8hNv2jIk"
      },
      "source": [
        "**Dendrogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waz4G8ea2xyZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[18,5])\n",
        "merg = linkage(dfc3, method='ward')\n",
        "dendrogram(merg, leaf_rotation=90,)\n",
        "plt.xlabel('Datapoints')\n",
        "plt.ylabel('Euclidean distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqUTU7PP21Sr"
      },
      "source": [
        "- We look for the largest distance that we can vertically observe without crossing any horizontal line.\n",
        "- We can observe from the above dendrogram that we can choose optimal clusters has 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdE75sB523-C"
      },
      "outputs": [],
      "source": [
        "## Building hierarchical clustering model using the optimal clusters as 2\n",
        "hie_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean',\n",
        "                                     linkage='ward')\n",
        "hie_cluster_model = hie_cluster.fit(dfc3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUOH0Y1K267r"
      },
      "outputs": [],
      "source": [
        "## Creating a dataframe of the labels\n",
        "df_label1 = pd.DataFrame(hie_cluster_model.labels_,columns=['Labels'])\n",
        "df_label1.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riTpli-N27Cn"
      },
      "outputs": [],
      "source": [
        "## joining the label dataframe with unscaled initial cleaned dataframe.(dfc1)\n",
        "\n",
        "df_hier = dfc1.join(df_label1)\n",
        "df_hier.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJN9oAIJ27Jh"
      },
      "outputs": [],
      "source": [
        "for i in range(2,15):\n",
        "    hier = AgglomerativeClustering(n_clusters=i)\n",
        "    hier = hier.fit(dfc3)\n",
        "    labels = hier.fit_predict(dfc3)\n",
        "    print(i,silhouette_score(dfc3,labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32mNqRcj3BN9"
      },
      "source": [
        "- We can observe from the silhouette scores for the agglomerative clustering for the 2 clusers the silhouette score is higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI09vN-Y3DyF"
      },
      "source": [
        "### 17. Conclusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUVQFwAe3HRp"
      },
      "source": [
        "Perform cluster analysis by doing bivariate analysis between cluster label and different features and write your\n",
        "conclusion on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbsoFp3S3KjU"
      },
      "outputs": [],
      "source": [
        "df_hier.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO6ZcF8r3KqQ"
      },
      "outputs": [],
      "source": [
        "df_hier['Labels'].value_counts().plot(kind='pie',autopct='%0.1f')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtUnZ2Zd3QMG"
      },
      "source": [
        "- We can observe that the clusters formed are imbalanced. There are more number of records assigned to cluster 0 than that of cluster 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92QGpA4t3S2D"
      },
      "outputs": [],
      "source": [
        "## Let us check the distribution of the different categories of 'rented for' column\n",
        "## w.r.t the clusters formed by agglomerative clustering technique.\n",
        "sns.countplot(df_hier['rented for'],hue='Labels',data=df_hier)\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9N3itnf3XcW"
      },
      "source": [
        "-  We can observe that there are more number of users who have rented the product is for 'wedding' and also there are more number of users belong to the cluster 0 compare to the cluster 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XDEUwSZ3YqX"
      },
      "outputs": [],
      "source": [
        "## Lets check the age distribution of the different clusters.\n",
        "sns.kdeplot(df_hier['age'],hue='Labels',data=df_hier)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nuSm3Kf3b0p"
      },
      "source": [
        "- The distribution of the age for different clusters is almost same, since there are more number of observations assigned to the cluster 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hpbzHfy3fmp"
      },
      "source": [
        "- In this project, we have attempted to implement and apply PCA on the renttherunway dataset and we selected 6 PCA compoments, which gave us the 90-95% of the variance in the data.\n",
        "- Also, we have used the PCA dimensions to cluster the data and segment the similar data in to clusters using KMeans clustering.\n",
        "- We have used Kmeans clustering algorithm to cluster the data, First we chose the optimal K value with the help of elbow plot used obtained K value from elbow plot to build a kmeans clustering model.\n",
        "- We have computed the silhoutte score for the different K values and evaluated the goodness of the clustering technique used.\n",
        "- We took the sample of the data and did agglomerative clustering using the original data and plotted dendrogram and analyzed the optimal number of classes and built a agglomerative clustering model using the obtained K value and evaluated the model using silhoutte score.\n",
        "- In this dataset, we had less number of features, further we can ask the company to collect the demographic information such as income and education. Geographic info such as where the customer is located rural or urban, city etc. Behavioral info such as browsing, spent amount by category, sentiment towards specific products and price points, and lastly the survey on lifestyle info such as hobbies, fashion etc.\n",
        "- By collecting more features, the customer segmentation/clustering of similar customers into groups will be more effective and we can infer more out of the clusters formed and will able to give suggestions to the company based on the analysis that will help the business to target the right customers and stand in the market for longer and make high revenue."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}